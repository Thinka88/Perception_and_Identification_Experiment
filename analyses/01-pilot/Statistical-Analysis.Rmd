---
title: "XPLab Perception of Randomness Experiment - Statistical Analysis"
author: "Mara Rehmer"
date: "1 7 2021"
output: html_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, cache = FALSE, message = FALSE, warning = FALSE, error = FALSE, fig.width = 5, fig.align = "center")

```

```{r libraries, message = FALSE, warning = FALSE, include = FALSE}

# check again, which libraries are really needed

# package for convenience functions (e.g. ggplot2, dplyr, etc.)
library(tidyverse)

# package for Bayesian regression modeling
library(brms)

# package for visualization
library(tidybayes)

# package to visualize 
library(bayesplot)

# these options help Stan run faster
options(mc.cores = parallel::detectCores())

#devtools::install_github("michael-franke/aida-package")
library(aida)

# use the aida-theme for plotting
theme_set(theme_aida())

# global color scheme / non-optimized
project_colors = c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#000000")

# setting theme colors globally
scale_colour_discrete <- function(...) {
  scale_colour_manual(..., values = project_colors)
}
scale_fill_discrete <- function(...) {
   scale_fill_manual(..., values = project_colors)
} 
```

## Loading and inspecting the data

First of all we load the data set and take a look at it.

```{r read_data}
data <- read_csv("results_271_XPLab+SS21+-+Group+13+-+Pilot_Mara,+Linus,+Cosima,+Katharina.csv")
glimpse(data)
```
First of all we have to exclude test trials. These were the first one after which we shared the link with our pilot participants. Then we had directly two problems with uploading the data to the server, which is why then two of us tried it out again. Meanwhile one set of data points was submitted. This is why we filter out these three trials.
```{r}
# exclude test trials
data <- data %>% 
  filter(submission_id != 2391, submission_id != 2395, submission_id != 2396)
```

### Participants

We will get a quick overview of the condition the participants took part in and their gender and age.
```{r}
d_summary <- data %>% 
  group_by(submission_id) %>% 
  summarize(gender = unique(gender),
            condition = unique(trial_name),
            age = unique(age))
            
d_summary
```

A total of `r d_summary$submission_id %>% length` participants took part in an online version of a Perception of Randomness task.
There were `r sum(d_summary$gender == 'male')` male, `r sum(d_summary$gender == 'female')` female and `r sum(d_summary$gender == 'other')` other participants, with ages ranging from `r d_summary$age %>% unique %>% min()` to `r d_summary$age %>% unique %>% max()`.



### General properties and cleaning

First we have a look at the comments:
```{r}
data$comments %>% unique
```


We check general properties of the data, like a summary of the reaction time (`RT`) and the overall time spent on the experiment (`timeSpent`).
```{r}
# check mean time spent
data %>% 
  pull(RT) %>% 
  summary()

# check overall time spent
data %>% 
  pull(timeSpent) %>% 
  summary()
```
The overall time spent on the experiment looks fine, but the maximal `RT` is quite high.

Now, we will clean the data by only selecting the columns that are relevant for us and getting rid of outliers.
First of all we select the columns that are relevant for our analysis:
`submission_id`: individual identifier of each subject  
`trial_name`: which we rename to `condition` which is either identification or discrimination  
`switch_rate`: ordered factor ranging from 0 to 1 in steps of 0.02 indicating the generating process of the stimuli  
`correctness`: whether the answer of the participant was correct  
`RT`: reaction time in ms for each trial (base line is 1500 ms from showing the stimulus)  
`timeSpent`: overall time spent  
`age`: age of the participant
`gender`: gender of the participant
`languages`: native language of the participant  
`comments`: question about whether the participant has an idea about what the experiment is about  


We choose to exclude individual trials with a reaction time (`RT`) bigger than 8000 ms, because we cannot be sure that participants recall the stimulus correctly after this time and just press any of the keys to continue to the next trial.
```{r data_wrangling}
# only include relevant columns
preprocessed_data <- data %>% 
  select(submission_id, trial_name, trial_number, switch_rate, correctness, RT, timeSpent, age, gender, languages, comments) %>% 
  # change trial_name to condition
  rename(condition = trial_name) %>% 
  mutate(outlier = case_when(RT > 8000 ~ TRUE,
                             TRUE ~ FALSE))

```

```{r}
# look at data points to exclude
ggplot(data = preprocessed_data, aes(x = trial_number, y = RT)) +
  geom_point(alpha = 0.2) +
  geom_point(data = filter(preprocessed_data, outlier == TRUE),
             color = "firebrick", shape = "square", size = 2)

```


```{r}
# exclude the outliers
clean_data <- filter(preprocessed_data, outlier == FALSE)
# number of outliers we exclude
preprocessed_data$outlier %>% sum()
```
We excluded `r preprocessed_data$outlier %>% sum()` trials for having too long reaction times.  
  
For now we will only need a few of the columns, so we will create a data set `clean_data` which only includes `experiment_id, condition, switch_rate, and correctness`.
```{r}
# make clean data without outliers, RT, timeSpent
clean_data <- preprocessed_data %>% 
  select(submission_id, condition, switch_rate, correctness)
```

We think that we have to make `switch_rate` an ordered factor and also treat it as character, because otherwise it is treated as a (continuous) numerical variable, but we need all levels to appear later to analyze them one by one.
```{r}
# make switch rate ordered factor
clean_data$switch_rate <- clean_data$switch_rate %>% 
  factor(ordered = TRUE)
```


### Summary statistics
Now we will have a look at some summary statistics.
We create our dependent variable `accuracy` by dummy coding the `correctness` variable and getting the mean for each condition first.
Then we get the accuracy for each `switch_rate` and `participant`.
Now we average the mean switch_rate accuracy over all participants and visualize it.

```{r}
# this is just to have the mean accuracy of each condition at hand
d <- clean_data %>% 
  group_by(condition) %>% 
  mutate(correct_response = ifelse(correctness == 'correct', 1, 0)) %>% 
  summarize(mean_accuracy = mean(correct_response))
d
```
This mean accuracy for both conditions look suspiciously low. But we will have to take a closer look at the data, to see what might have gone wrong.

```{r}
# get overall accuracy for each condition (the same as above)
clean_data <- clean_data %>% 
  group_by(condition) %>% 
  mutate(correct_response = ifelse(correctness == 'correct', 1, 0),
         condition_accuracy = mean(correct_response)) %>% 
  ungroup() %>% 
  # get accuracy for each switch rate and participant individually 
  group_by(submission_id, switch_rate) %>% 
  mutate(individual_swr_accuracy = mean(correct_response)) %>% 
  ungroup() %>% 
  # average individual switch_rate over all participants
  group_by(switch_rate) %>% 
  mutate(average_swr_accuracy = mean(individual_swr_accuracy)) %>% 
  ungroup() %>% 
  group_by(condition, submission_id, switch_rate)
  
```

For both conditions (**This we still have to do!!!**) we plot all data points of the participants with geom_point with a low alpha level just to see, how far they spread out. We use geom_smooth with method "auto" to fit a smooth line to the data points. We also include a red horizontal line at accuracy 0.5 which is just chance.
```{r}
ggplot(data = clean_data, aes(x = switch_rate, y = average_swr_accuracy)) +
  geom_point(aes(y = individual_swr_accuracy), alpha = 0.01) +
  # use this to have smooth line
  geom_smooth(method = "auto") +
  # use this to have accurate line
  #geom_line() +
  # indicator for chance
  geom_hline(aes(yintercept = 0.5), color = "darkred")
  # something with facet_grid to display both conditions
  # but this down below does not seem to work
  #facet_wrap(~ condition, ncol = 1)
  
```

Looking at this plot reveals that either each of the pilot participants mixed up horizontal and vertical, or we messed up in our code of the experiment in the `correctness` variable. Looking at the experiment code we quickly find, that we had both expected keys mixed up. To not lose the collected data due to our mistakes, we choose to take 1 minus the accuracy results. This will lead to the correct representation.


```{r}
# fix our error by taking the three participants and inversing their accuracy
clean_data_fixing_our_error <- clean_data %>% 
  filter(submission_id == 2394 | submission_id == 2398 | submission_id == 2400) %>% 
  select(-condition_accuracy , -individual_swr_accuracy, -average_swr_accuracy) %>% 
  mutate(correctness = ifelse(correctness == 'correct', 'incorrect', 'correct'))

# all of the data collected after we fixed our error
clean_data <- clean_data %>% 
  filter(submission_id != 2394, submission_id != 2398, submission_id != 2400) %>% 
  select(-condition_accuracy, -individual_swr_accuracy, -average_swr_accuracy)
```

```{r}
clean_data <- full_join(clean_data_fixing_our_error, clean_data)
```

```{r}
clean_data <- clean_data %>% 
  ungroup() %>% 
  group_by(condition) %>% 
  mutate(correct_response = ifelse(correctness == 'correct', 1, 0),
    condition_accuracy = mean(correct_response)) %>% 
  ungroup() %>% 
  # get accuracy for each switch rate and participant individually 
  group_by(submission_id, switch_rate) %>% 
  mutate(individual_swr_accuracy = mean(correct_response)) %>% 
  ungroup() %>% 
  # average individual switch_rate over all participants
  group_by(switch_rate) %>% 
  mutate(average_swr_accuracy = mean(individual_swr_accuracy)) %>% 
  ungroup() %>% 
  group_by(condition, submission_id, switch_rate)
```

Now we have another look at the overall accuracy:

```{r}
ggplot(data = clean_data, aes(x = switch_rate, y = average_swr_accuracy)) +
  geom_point(aes(y = individual_swr_accuracy), alpha = 0.01) +
  # use this to have smooth line
  geom_smooth(method = "auto") +
  # use this to have accurate line
  #geom_line() +
  # indicator for chance
  geom_hline(aes(yintercept = 0.5), color = "darkred")
  # something with facet_grid to display both conditions
  # but this down below does not seem to work
  #facet_wrap(~ condition, ncol = 1)
  
```

```{r}
#clean_data$condition_accuracy %>% unique()
unique(clean_data$condition_accuracy)[2]
```
This looks a lot more plausible. We can now continue.


## Hypothesis

The hypothesis of the original paper states 
> The probability of correctly identifying stimuli from R [random source] and N [non-random source] coincides with the ease of distinguishing between the two sources.


## Analysis

It is a 2x51 mixed factorial design, where the first factor is `condition` with two values *discrimination* or *identification*. The second factor is `switch rate` which has 51 levels. Each participant contributes data points for only one condition (between-subjects) but multiple data points (ideally 10) for each switch rate (within-subject, repeated measures).

We will analyze the `average_swr_accuracy` as dependent variable in terms of the condition and switch rate as independent variables.
The accuracy is determined as the mean of correct responses for each switch rate. This is calculated for each participant in each condition and then averaged over all participants in the respective condition (as done above).
In our bayesian regression model we also want to include random effects for each participant because the linear regression model assumes that the data points are independent of each other. But each participant contributes multiple data points for each switch rate. But we are not sure about them. 

```{r}
model_1 <- brm(data = clean_data,
             seed = 13,
             # not sure about the random effects
             formula = average_swr_accuracy ~ condition + switch_rate + (1|submission_id)
             )
model_1
```
Looking at the results there are many parameters. We are not sure why the first three switch_rates are labeled with L, Q, and C.
We tried the same model with added random effects (switch_rate || submission_id) but the model did not converge at all.
Looking at the Rhat value for family specific parameters and the group level effects we can see that the model did not converge for them.
The credible interval for `conditionidentification` ranges from -0.14 to 0.31, that includes zero, so for the amount of data so far, we cannot say that there is a credible difference between the two conditions.

Generally, we are not sure, whether the model above is even fitting for our "problem". 
We also had the idea to use a logistic regression model since `correctness` is a binary variable and accuracy depends on it. The one we use below does not account for the within subject factor `switch_rate`, because we don't know how to include it, or if that is even possible/necessary.
We use weakly informative priors class b coefficients, precisely student_t distribution with $\mu = 0, \sigma = 1, \nu = 1$.


```{r}
logistic_regression <- brm(
  seed = 13,
  formula = correctness ~ condition,
  family = bernoulli(link = "logit"),
  data = clean_data %>% 
    mutate(correctness = correctness == 'correct'),
  prior = prior(student_t(1, 0, 1), class = 'b'), 
  sample_prior = 'yes',
  iter = 20000)
```


```{r}
pp_check(logistic_regression)
```

```{r}
logistic_regression
```
```{r}
# predictor of central tendency
logistic <- function(x){
  return((1+exp(-x))^-1)
}

intercept <- fixef(logistic_regression)[1,1]
conditionIdentification <- fixef(logistic_regression)[2,1]

estimates <- tribble(~estimates_from, ~discrimination, ~identification,
                     "model", logistic(intercept), logistic(intercept+conditionIdentification),
                     "data", unique(clean_data$condition_accuracy)[1], unique(clean_data$condition_accuracy)[2]
                     )
estimates

```

The pp_check seems to be fitting. And also the linear predictors when transformed to central predictors seem reasonable.
Now we test the hypothesis that the condition has no effect on the probability of correctly identifying the different stimuli.

```{r}
brms::hypothesis(logistic_regression, "conditionidentification = 0")
```

For the point-valued hypothesis the value of the estimate and the corresponding 95% credible interval are all negative numbers. The evidence ratio (Bayes factor) is 0.19 and the posterior probability that the hypothesis is true is 0.16 given the data and the model. The star indicates that the value tested against lies outside the 95%-CI, which is why we reject the hypothesis.


```{r}
brms::hypothesis(logistic_regression, "conditionidentification < 0")
```
For this one-sided hypothesis that accuracy in the identification condition is lower than in the discrimination condition, the evidence ratio (posterior odds) are 85.21 and the posterior probability exceeds 95%, which is why this hypothesis is more likely to be true given the model and the data.



In this model below we tried to incorporate the switch_rate but the session always crashes.
```{r}
#glm_logistic_regression_w_swr <- brm(
#  formula = correctness ~ condition + switch_rate,
#  family = bernoulli(link = "logit"),
#  data = clean_data %>% 
#    mutate(correctness = correctness == 'correct'),
#  prior = prior(student_t(1, 0, 1), class = 'b'),
#  sample_prior = 'yes',
#  iter = 20000)
```

